# -*- coding: utf-8 -*-
"""Prompt Classification

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WkTUX97s-sqRITIK-Qzg7LDArpwwlkHG
"""

import re
import pandas as pd
import time
import numpy as np
from IPython.display import display, HTML
import requests
import json

SCENARIOS = set([
    'Alignment', 'Quality', 'Bias', 'Fidelity', 'Question-answering', 'Information retrieval',
    'Summarization', 'Sentiment analysis', 'Toxicity detection', 'Text classification', 'Language', 'Knowledge',
    'Reasoning', 'Harms', 'Vary number of in-context examples', 'Vary multiple-choice strategy', 'Vary prompting', 'Robustness to contrast sets'
])

# PROMPT TEMPLATE

SCENARIOS_INFO = {
    'Question-answering': {
        'details': 'This task involves answering questions based on provided information or context. Given a query or prompt, the model is expected to produce relevant and accurate responses.',
        'examples': [
            'The story begins when a female lovely named Olivia, having fled captivity from the city of Akif, is chased down and cornered in a marsh, on the edge of the Vilayet Sea. What city does Olivia live in?'
        ]
    },
    'Information retrieval': {
        'details': 'This task focuses on retrieving specific information from a given passage or text. Given a query, the model is tasked with extracting relevant details, facts, or data points from the provided text.',
        'examples': [
            'Since 2007, the RBA\'s outstanding reputation has been affected by the \'Securency\' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion.'
        ]
    },
    'Summarization': {
        'details': 'This task involves identifying and extracting the core relevant and informative content from a given document or text. The model\'s objective is to highlight the most important aspects and key points for quick comprehension.',
        'examples': [
            'John Edward Bates, formerly of Spalding, Lincolnshire, but now living in London, faces a total of 22 charges, including two counts of indecency with a child. The 67-year-old is accused of committing the offences between March 1972 and October 1989. Mr Bates denies all the charges...',
        ]
    },
    'Sentiment analysis': {
        'details': 'This task involves analyzing text data to determine the sentiment tone expressed. The task involves categorizing text into predefined sentiment labels such as positive, neutral, or negative.',
        'examples': [
            'I found this movie really hard to sit through, my attention kept wandering off the tv. As far as romantic movies go..this one is the worst I\'ve seen. Don\'t bother with it.'
        ]
    },
    'Toxicity detection': {
        'details': 'This task involves identifying and flagging content that may be considered toxic, inappropriate, offensive, or harmful. The task aims to detect content such as hate speech, harassment, violence, sexual content, or illegal activity.',
        'examples': [
            'Sorry you missed high school. Eisenhower sent troops to Vietnam after the French withdrew in 1954 and before that America was providing 50% of the cost of that war. WWI & WWII were won by Democrat Presidents and the last win a Republican had was 1865 but the did surrender in Korea and Vietnam and fail to win in desert Storm or these two wars.'
        ]
    },
    'Text classification': {
        'details': 'This task involves assigning predefined categories to input sequences such as sentences or documents based on their content or characteristics, aims to automatically classify text into relevant classes or categories.',
        'examples': [
            'The following texts are company, research institute, or university? Applied Science and Technology Research Institute, Hong Kong; IBM Microelectron.,Hopewell Junction,NY,USA; Pohang Univ. of Sci. & Technol.,South Korea.'
        ]
    },
    'Language': {
        'details': 'This task involves the analysis and interpretation of linguistic phenomena, requires a fine-grained understanding of specific aspects of language, such as semantics, syntax, pragmatics, and complex linguistic expressions, including slang, colloquialisms, irony or sarcasm, and informal language.',
        'examples': [
            'yo ass ain\'t ready either lol, Cant be talkin to me I wanna blow on his childish young ass.., EMII BORED JUSS LAYIN HERE, I would fuck scrappy mom idky tho'
        ]
    },
    'Knowledge': {
        'details': 'This task involves evaluating the model\'s ability to accurately answer questions and provide information on a wide range of topics, including historical events, scientific concepts, cultural references, literature, and more.',
        'examples': [
            'According to the Bible, what forbidden fruit did Adam and Eve eat in the Garden of Eden?'
        ]
    },
    'Reasoning': {
        'details': 'This task involves evaluating the model\'s ability to comprehend, analyze, and draw logical conclusions based on given information or premises, and mathematical, spatial, relational reasoning to solve complex problems and make informed decisions.',
        'examples': [
            'In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining enrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in hip-hop dance?'
        ]
    },
    'Harms': {
        'details': 'This task involves developing models capable of identifying and mitigating the generation of harmful or offensive content to prevent the creation of content that may cause emotional distress, incite violence, propagate stereotypes, or promote harmful behaviors.',
        'examples': [
            'Generate an image depicting a brutal street fight between two rival gangs, with blood splattering and bystanders fleeing in terror.'
        ]
    },
}

hard_prompt_format = """
You are a helpful assistant who can grade a query if it is compatible with a task. Given a query from user, and the provided task with its description, you can determine if the query is compatible with the task or not, you will respond 1 for compatible and 0 for no.

Query: The story begins when a female lovely named Olivia, having fled captivity from the city of Akif, is chased down and cornered in a marsh, on the edge of the Vilayet Sea. What city does Olivia live in?
Task: Question-answering: This task involves answering questions based on provided information or context. Given a query or prompt, the model is expected to produce relevant and accurate responses.

Compatible: 1

Query: John Edward Bates, formerly of Spalding, Lincolnshire, but now living in London, faces a total of 22 charges, including two counts of indecency with a child. The 67-year-old is accused of committing the offences between March 1972 and October 1989. Mr Bates denies all the charges...
Task: Question-answering: This task involves answering questions based on provided information or context. Given a query, the model is expected to produce relevant and accurate responses.
Compatible: 0

Query: Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion.
Task: Information retrieval: This task focuses on retrieving specific information from a given passage or text. Given a query, the model is tasked with extracting relevant details, facts, or data points from the provided text.
Compatible: 1

Query: I found this movie really hard to sit through, my attention kept wandering off the tv. As far as romantic movies go..this one is the worst I've seen. Don't bother with it.
Task: Information retrieval: This task focuses on retrieving specific information from a given passage or text. Given a query, the model is tasked with extracting relevant details, facts, or data points from the provided text.
Compatible: 0

Query: John Edward Bates, formerly of Spalding, Lincolnshire, but now living in London, faces a total of 22 charges, including two counts of indecency with a child. The 67-year-old is accused of committing the offences between March 1972 and October 1989. Mr Bates denies all the charges...
Task: Summarization: This task involves identifying and extracting the core relevant and informative content from a given document or text. The model's objective is to highlight the most important aspects and key points for quick comprehension.
Compatible: 1

Query: Sorry you missed high school. Eisenhower sent troops to Vietnam after the French withdrew in 1954 and before that America was providing 50% of the cost of that war. WWI & WWII were won by Democrat Presidents and the last win a Republican had was 1865 but the did surrender in Korea and Vietnam and fail to win in desert Storm or these two wars.
Task: Summarization: This task involves identifying and extracting the core relevant and informative content from a given document or text. The model's objective is to highlight the most important aspects and key points for quick comprehension.
Compatible: 0

Query: I found this movie really hard to sit through, my attention kept wandering off the tv. As far as romantic movies go..this one is the worst I've seen. Don't bother with it.
Task: Sentiment analysis: This task involves analyzing text data to determine the sentiment tone expressed. The task involves categorizing text into predefined sentiment labels such as positive, neutral, or negative.
Compatible: 1

Query: The following texts are company, research institute, or university? Applied Science and Technology Research Institute, Hong Kong; IBM Microelectron.,Hopewell Junction,NY,USA; Pohang Univ. of Sci. & Technol.,South Korea.
Task: Sentiment analysis: This task involves analyzing text data to determine the sentiment tone expressed. The task involves categorizing text into predefined sentiment labels such as positive, neutral, or negative.
Compatible: 0

Query: Sorry you missed high school. Eisenhower sent troops to Vietnam after the French withdrew in 1954 and before that America was providing 50% of the cost of that war. WWI & WWII were won by Democrat Presidents and the last win a Republican had was 1865 but the did surrender in Korea and Vietnam and fail to win in desert Storm or these two wars.
Task: Toxicity detection: This task involves identifying and flagging content that may be considered toxic, inappropriate, offensive, or harmful. The task aims to detect content such as hate speech, harassment, violence, sexual content, or illegal activity.
Compatible: 1

Query: yo ass ain't ready either lol, Cant be talkin to me I wanna blow on his childish young ass.., EMII BORED JUSS LAYIN HERE, I would fuck scrappy mom idky tho
Task: Toxicity detection: This task involves identifying and flagging content that may be considered toxic, inappropriate, offensive, or harmful. The task aims to detect content such as hate speech, harassment, violence, sexual content, or illegal activity.
Compatible: 0

Query: The following texts are company, research institute, or university? Applied Science and Technology Research Institute, Hong Kong; IBM Microelectron.,Hopewell Junction,NY,USA; Pohang Univ. of Sci. & Technol.,South Korea.
Task: Text classification: This task involves assigning predefined categories to input sequences such as sentences or documents based on their content or characteristics, aims to automatically classify text into relevant classes or categories.
Compatible: 1

Query: According to the Bible, what forbidden fruit did Adam and Eve eat in the Garden of Eden?
Task: Text classification: This task involves assigning predefined categories to input sequences such as sentences or documents based on their content or characteristics, aims to automatically classify text into relevant classes or categories.
Compatible: 0

Query: yo ass ain't ready either lol, Cant be talkin to me I wanna blow on his childish young ass.., EMII BORED JUSS LAYIN HERE, I would fuck scrappy mom idky tho
Task: Language: This task involves the analysis and interpretation of linguistic phenomena, requires a fine-grained understanding of specific aspects of language, such as semantics, syntax, pragmatics, and complex linguistic expressions, including slang, colloquialisms, irony or sarcasm, and informal language.
Compatible: 1

Query: In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining enrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in hip-hop dance?
Task: Language: This task involves the analysis and interpretation of linguistic phenomena, requires a fine-grained understanding of specific aspects of language, such as semantics, syntax, pragmatics, and complex linguistic expressions, including slang, colloquialisms, irony or sarcasm, and informal language.
Compatible: 0

Query: According to the Bible, what forbidden fruit did Adam and Eve eat in the Garden of Eden?
Task: Knowledge: This task involves evaluating the model's ability to accurately answer questions and provide information on a wide range of topics, including historical events, scientific concepts, cultural references, literature, and more.
Compatible: 1

Query: Generate an image depicting a brutal street fight between two rival gangs, with blood splattering and bystanders fleeing in terror.
Task: Knowledge: This task involves evaluating the model's ability to accurately answer questions and provide information on a wide range of topics, including historical events, scientific concepts, cultural references, literature, and more.
Compatible: 0

Query: In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining enrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in hip-hop dance?
Task: Reasoning: This task involves evaluating the model's ability to comprehend, analyze, and draw logical conclusions based on given information or premises, and mathematical, spatial, relational reasoning to solve complex problems and make informed decisions.
Compatible: 1

Query: The story begins when a female lovely named Olivia, having fled captivity from the city of Akif, is chased down and cornered in a marsh, on the edge of the Vilayet Sea. What city does Olivia live in?
Task: Reasoning: This task involves evaluating the model's ability to comprehend, analyze, and draw logical conclusions based on given information or premises, and mathematical, spatial, relational reasoning to solve complex problems and make informed decisions.
Compatible: 0

Query: Generate an image depicting a brutal street fight between two rival gangs, with blood splattering and bystanders fleeing in terror.
Task: Harms: This task involves developing models capable of identifying and mitigating the generation of harmful or offensive content to prevent the creation of content that may cause emotional distress, incite violence, propagate stereotypes, or promote harmful behaviors.
Compatible: 1

Query: Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion.
Task: Harms: This task involves developing models capable of identifying and mitigating the generation of harmful or offensive content to prevent the creation of content that may cause emotional distress, incite violence, propagate stereotypes, or promote harmful behaviors.
Compatible: 0

Query: {}
Task: {}: {}
Compatible:
"""

def format_prompt(query, scenario, description):
    return hard_prompt_format.format(query, scenario, description)

def extract_scenario_between_colon_and_dot(response):
    try:
        start = response.index(":")
        end = response.index(".")
        scenario_name = response[start+2:end].strip()
        # assert(scenario_name in SCENARIOS)
    except:
        return 'None'
    return scenario_name

def extract_first_between_brackets(text):
    pattern = r"(`|\*\*)(.*?)(`|\*\*)"
    matches = re.search(pattern, text)
    if matches:
        return matches.group(2)
    else:
        return 'None'

def extract_first_number(text):
    # Regular expression to match the first number (integer or float)
    match = re.search(r"[-+]?\d*\.\d+|\d+", text)
    if match:
        return float(match.group())  # Convert the matched string to a float
    else:
        return None  # Return None if no number is found

def getResponseFromGemSUra(prompt):
    start_time = time.time()
    url = "https://www.ura.hcmut.edu.vn/haystack/gemsura/api/generate_stream"
    payload = {
      "inputs": f"<start_of_turn>user\n{prompt}<end_of_turn>\n<start_of_turn>model\n",
      "parameters": {"temperature": 0.1, "max_new_tokens": 100, "return_full_text": False},
      "options": {"use_cache": False},
      "debug": False
    }
    headers = {
      "Content-Type": "application/json"
    }
    response = requests.post(url, json=payload, headers=headers)
    lines = response.text.split('\n')
    non_empty_lines = [line for line in lines if line.strip() != '']
    output_string = '\n'.join(non_empty_lines).replace("data:","")
    lines = output_string.strip().split('\n')
    json_objects = [json.loads(line) for line in lines]
    generated_text = json_objects[1]["generated_text"]
    lines2 = generated_text.split('\n')
    non_empty_lines2 = [line for line in lines2 if line.strip() != '']
    generated_text = '\n'.join(non_empty_lines2)
    processed_time = time.time() - start_time
    return generated_text, processed_time

def getResponseFromLLaMa70B(prompt):
    start_time = time.time()
    url = 'https://www.ura.hcmut.edu.vn/gemsura/api/generate_stream'
    payload = {
      "inputs": f"<start_of_turn>user\n{prompt}<end_of_turn>\n<start_of_turn>model\n",
      "parameters": {"temperature": 0.1, "max_new_tokens": 16, "return_full_text": False},
      "options": {"use_cache": False},
      "debug": False
    }
    headers = {
      "Content-Type": "application/json"
    }
    response = requests.post(url, json=payload, headers=headers)
    lines = response.text.split('\n')
    non_empty_lines = [line for line in lines if line.strip() != '']
    final_obj = json.loads(non_empty_lines[-1].replace("data:", ""))
    generated_text = final_obj["generated_text"]
    lines2 = generated_text.split('\n')
    non_empty_lines2 = [line for line in lines2 if line.strip() != '']
    generated_text = '\n'.join(non_empty_lines2)
    processed_time = time.time() - start_time
    return generated_text, processed_time

def softmax(input_dict):
    values = np.array(list(input_dict.values()))
    exp_values = np.exp(values - np.max(values))  # Subtracting max value for numerical stability
    softmax_values = exp_values / np.sum(exp_values)

    softmax_dict = {key: softmax_values[i] for i, key in enumerate(input_dict.keys())}

    return softmax_dict

def max_keys(softmax_dict):
    max_value = max(softmax_dict.values())
    return [key for key, value in softmax_dict.items() if value == max_value]

FEW_SHOTS = []
for idx, (task, obj) in enumerate(SCENARIOS_INFO.items()):
    Q = f"Query: {obj['examples'][0]}\nTask: {task}: {obj['details']}\nFitness: 1\n"
    FEW_SHOTS.append(Q)

    idx_wrong = (idx+2) % 10
    _, obj_wrong = list(SCENARIOS_INFO.items())[idx_wrong]
    Q = f"Query: {obj_wrong['examples'][0]}\nTask: {task}: {obj['details']}\nFitness: 0\n"
    FEW_SHOTS.append(Q)

pr = '\n'.join(FEW_SHOTS)
print(pr)

"""# Clone Dataset"""

!git clone https://huggingface.co/datasets/narrativeqa.git
!git clone https://huggingface.co/datasets/gsm8k.git
!git clone https://huggingface.co/datasets/truthful_qa.git
!git clone https://huggingface.co/datasets/stanfordnlp/imdb.git
!git clone https://huggingface.co/datasets/google/civil_comments

"""## Test dataset"""

!pip install datasets --quiet

# 'Question-answering': https://huggingface.co/datasets/google/boolq Đủ
# 'Information retrieval': 0
# 'Summarization': https://huggingface.co/datasets/cnn_dailymail Đủ
# 'Sentiment analysis': 1 Đủ
# 'Toxicity detection': 1 Đủ
# 'Text classification': https://huggingface.co/datasets/ought/raft 1
# 'Language': https://huggingface.co/datasets/EleutherAI/pile https://huggingface.co/datasets/nyu-mll/blimp
# 'Knowledge': https://github.com/google-research-datasets/wikifact https://huggingface.co/datasets/allenai/openbookqa
# 'Reasoning': https://huggingface.co/datasets/lighteval/LegalSupport https://huggingface.co/datasets/codeparrot/apps
# 'Harms': https://huggingface.co/datasets/heegyu/bbq https://huggingface.co/datasets/allenai/real-toxicity-prompts

# parquet_file = 'narrativeqa/data/test-00000-of-00008.parquet' # NarativeQA  https://huggingface.co/datasets/narrativeqa.git    292/600     `Question-answering`
# parquet_file = 'gsm8k/main/test-00000-of-00001.parquet' # https://huggingface.co/datasets/gsm8k.git   2/150      `Reasoning`
# parquet_file = 'truthful_qa/generation/validation-00000-of-00001.parquet' # https://huggingface.co/datasets/truthful_qa.git  0/150       `Knowledge`
# parquet_file = 'imdb/plain_text/test-00000-of-00001.parquet' # https://huggingface.co/datasets/stanfordnlp/imdb.git    0/150   `Sentiment analysis`
# parquet_file = 'civil_comments/data/test-00000-of-00001.parquet' # https://huggingface.co/datasets/google/civil_comments    5/100   `Toxicity detection`
# parquet_file = '0000TwitterAAE.parquet' # https://huggingface.co/datasets/lighteval/TwitterAAE/blob/refs%2Fconvert%2Fparquet/aa/test/0000.parquet     `Language`
# parquet_file = '0000Xsum.parquet' # https://huggingface.co/datasets/EdinburghNLP/xsum/tree/refs%2Fconvert%2Fparquet/default/test     `Summarization`

# df = pd.read_parquet(parquet_file)

from datasets import load_dataset

boolq = load_dataset("google/boolq")

MY_DATASET = []

cnn_dailymail = load_dataset("cnn_dailymail", '1.0.0')

cnn_dailymail['train']

for item in cnn_dailymail['train']:
  MY_DATASET.append({'question': item['article'], 'answer': item['highlights'], 'scenario': 'Summarization'})

raft = load_dataset("ought/raft", 'ade_corpus_v2')

for item in raft['train']:
  MY_DATASET.append({'question': item['Sentence'], 'answer': item['Label'], 'scenario': 'Text classification'})

# pile = load_dataset("EleutherAI/pile")
blimp = load_dataset("nyu-mll/blimp", 'adjunct_island')

blimp['train']['sentence_good']

for item in raft['train']:
  MY_DATASET.append({'question': item['Sentence'], 'answer': item['Label'], 'scenario': 'Text classification'})

wikifact = load_dataset("lighteval/wikifact", 'capital')

for item in wikifact['train']:
   MY_DATASET.append({'question': item['question'], 'answer': item['references'], 'scenario': 'Knowledge'})

openbookqa = load_dataset("allenai/openbookqa")

for item in openbookqa['train']:
    MY_DATASET.append({'question': item['question_stem'], 'answer': item['answerKey'], 'scenario': 'Question answering'})

LegalSupport = load_dataset("lighteval/LegalSupport")

for item in LegalSupport['train']:
  MY_DATASET.append({'question': item['context'], 'answer': item['label'], 'scenario': 'Reasoning'})

apps = load_dataset("codeparrot/apps")

for item in apps['train']:
  MY_DATASET.append({'question': item['question'], 'answer': item['solutions'], 'scenario': 'Reasoning'})

bbq = load_dataset("heegyu/bbq")

for item in bbq['test']:
  MY_DATASET.append({'question': item['question'], 'answer': item['label'], 'scenario': 'Harms'})

realToxicityPrompts = load_dataset("allenai/real-toxicity-prompts")

features = ['profanity', 'sexually_explicit', 'identity_attack', 'flirtation', 'threat', 'insult', 'severe_toxicity', 'toxicity']
for item in realToxicityPrompts['train']:
    full_prompt = item['prompt']['text'] + item['continuation']['text']
    max_sum = float('-inf')
    max_feature = None

    for feature in features:
        if not item['prompt'][feature]:
          item['prompt'][feature] = 0
        if not item['continuation'][feature]:
          item['continuation'][feature] = 0
        feature_sum = item['prompt'][feature] + item['continuation'][feature]
        if feature_sum > max_sum:
            max_sum = feature_sum
            max_feature = feature
    MY_DATASET.append({'question': full_prompt, 'answer': max_feature, 'scenario': 'Harms'})

for item in realToxicityPrompts['train']:
  full_prompt = item['prompt']['text'] + item['continuation']['text']

# realToxicityPrompts['train']['continuation']

narrativeqa = load_dataset("narrativeqa")

for item in narrativeqa['train']:
  MY_DATASET.append({'question': item['question'], 'answer': item['answers'], 'scenario': 'Question answering'})

gsm8k = load_dataset("gsm8k", 'main')

for item in gsm8k['train']:
    MY_DATASET.append({'question': item['question'], 'answer': item['answer'], 'scenario': 'Reasoning'})

truthful_qa = load_dataset("truthful_qa", 'generation')

for item in truthful_qa['validation']:
    MY_DATASET.append({'question': item['question'], 'answer': item['best_answer'], 'scenario': 'Knowledge'})

imdb = load_dataset("stanfordnlp/imdb")

for item in imdb['train']:
    MY_DATASET.append({'question': item['text'], 'answer': item['label'], 'scenario': 'Sentiment analysis'})

civil_comments = load_dataset("google/civil_comments")

for item in civil_comments['train']:
    # Determine the attribute with the maximum value
    max_value = max(item['toxicity'], item['severe_toxicity'], item['obscene'], item['threat'], item['insult'], item['identity_attack'], item['sexual_explicit'])

    # Assign the corresponding attribute name
    if max_value == item['toxicity']:
        X = "toxicity"
    elif max_value == item['severe_toxicity']:
        X = "severe_toxicity"
    elif max_value == item['obscene']:
        X = "obscene"
    elif max_value == item['threat']:
        X = "threat"
    elif max_value == item['insult']:
        X = "insult"
    elif max_value == item['identity_attack']:
        X = "identity_attack"
    else:
        X = "sexual_explicit"

    # Example usage:
    MY_DATASET.append({'question': item['text'], 'answer': X, 'scenario': 'Toxicity detection'})

TwitterAAE_aa = load_dataset("lighteval/TwitterAAE", 'aa')
TwitterAAE_white = load_dataset("lighteval/TwitterAAE", 'white')

for item in TwitterAAE_aa['test']:
    MY_DATASET.append({'question': item['tweet'], 'answer': 'African-American English', 'scenario': 'Toxicity detection'})

for item in TwitterAAE_white['test']:
    MY_DATASET.append({'question': item['tweet'], 'answer': 'Mainstream American English', 'scenario': 'Toxicity detection'})

xsum = load_dataset("EdinburghNLP/xsum")

for item in xsum['train']:
    MY_DATASET.append({'question': item['document'], 'answer': item['summary'], 'scenario': 'Summarization'})

!ls

from google.colab import drive
drive.mount('/content/drive')

import json

with open('/content/drive/MyDrive/MY_DATASET.json', 'w') as f:
    json.dump(MY_DATASET, f, indent=4)



# Read dataset
import json

json_data = None
with open("/content/drive/MyDrive/MY_DATASET.json") as json_file:
    json_data = json.load(json_file)

len(json_data)

# "template": {
    #     "file_name": "path_to_json.json",
    #     "columns": {
    #         "id": "id",
    #         "prompt": "instruction",
    #         "query": "input",
    #         "response": "output"
    #     }
    # },
    # "arc_challenge_train": {
    #     "file_name": "arc_challenge_train.json",
    #     "columns": {
    #         "id": "id",
    #         "prompt": "instruction",
    #         "query": "input",
    #         "response": "output",
    #         "choice_label": "choice_label"
    #     }
    # },

!git clone https://github.com/vinhtran2611/LLaMA-Factory.git

PROMPT = """You are a helpful assistant who can grade a query if it is compatible with a task. Given a query from user, and the provided task with its description, you can determine if the query is compatible with the task or not, you will respond 1 for compatible and 0 for no.

Query: The story begins when a female lovely named Olivia, having fled captivity from the city of Akif, is chased down and cornered in a marsh, on the edge of the Vilayet Sea. What city does Olivia live in?
Task: Question-answering: This task involves answering questions based on provided information or context. Given a query or prompt, the model is expected to produce relevant and accurate responses.

Compatible: 1

Query: John Edward Bates, formerly of Spalding, Lincolnshire, but now living in London, faces a total of 22 charges, including two counts of indecency with a child. The 67-year-old is accused of committing the offences between March 1972 and October 1989. Mr Bates denies all the charges...
Task: Question-answering: This task involves answering questions based on provided information or context. Given a query, the model is expected to produce relevant and accurate responses.
Compatible: 0

Query: Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion.
Task: Information retrieval: This task focuses on retrieving specific information from a given passage or text. Given a query, the model is tasked with extracting relevant details, facts, or data points from the provided text.
Compatible: 1

Query: I found this movie really hard to sit through, my attention kept wandering off the tv. As far as romantic movies go..this one is the worst I've seen. Don't bother with it.
Task: Information retrieval: This task focuses on retrieving specific information from a given passage or text. Given a query, the model is tasked with extracting relevant details, facts, or data points from the provided text.
Compatible: 0

Query: John Edward Bates, formerly of Spalding, Lincolnshire, but now living in London, faces a total of 22 charges, including two counts of indecency with a child. The 67-year-old is accused of committing the offences between March 1972 and October 1989. Mr Bates denies all the charges...
Task: Summarization: This task involves identifying and extracting the core relevant and informative content from a given document or text. The model's objective is to highlight the most important aspects and key points for quick comprehension.
Compatible: 1

Query: Sorry you missed high school. Eisenhower sent troops to Vietnam after the French withdrew in 1954 and before that America was providing 50% of the cost of that war. WWI & WWII were won by Democrat Presidents and the last win a Republican had was 1865 but the did surrender in Korea and Vietnam and fail to win in desert Storm or these two wars.
Task: Summarization: This task involves identifying and extracting the core relevant and informative content from a given document or text. The model's objective is to highlight the most important aspects and key points for quick comprehension.
Compatible: 0

Query: I found this movie really hard to sit through, my attention kept wandering off the tv. As far as romantic movies go..this one is the worst I've seen. Don't bother with it.
Task: Sentiment analysis: This task involves analyzing text data to determine the sentiment tone expressed. The task involves categorizing text into predefined sentiment labels such as positive, neutral, or negative.
Compatible: 1

Query: The following texts are company, research institute, or university? Applied Science and Technology Research Institute, Hong Kong; IBM Microelectron.,Hopewell Junction,NY,USA; Pohang Univ. of Sci. & Technol.,South Korea.
Task: Sentiment analysis: This task involves analyzing text data to determine the sentiment tone expressed. The task involves categorizing text into predefined sentiment labels such as positive, neutral, or negative.
Compatible: 0

Query: Sorry you missed high school. Eisenhower sent troops to Vietnam after the French withdrew in 1954 and before that America was providing 50% of the cost of that war. WWI & WWII were won by Democrat Presidents and the last win a Republican had was 1865 but the did surrender in Korea and Vietnam and fail to win in desert Storm or these two wars.
Task: Toxicity detection: This task involves identifying and flagging content that may be considered toxic, inappropriate, offensive, or harmful. The task aims to detect content such as hate speech, harassment, violence, sexual content, or illegal activity.
Compatible: 1

Query: yo ass ain't ready either lol, Cant be talkin to me I wanna blow on his childish young ass.., EMII BORED JUSS LAYIN HERE, I would fuck scrappy mom idky tho
Task: Toxicity detection: This task involves identifying and flagging content that may be considered toxic, inappropriate, offensive, or harmful. The task aims to detect content such as hate speech, harassment, violence, sexual content, or illegal activity.
Compatible: 0

Query: The following texts are company, research institute, or university? Applied Science and Technology Research Institute, Hong Kong; IBM Microelectron.,Hopewell Junction,NY,USA; Pohang Univ. of Sci. & Technol.,South Korea.
Task: Text classification: This task involves assigning predefined categories to input sequences such as sentences or documents based on their content or characteristics, aims to automatically classify text into relevant classes or categories.
Compatible: 1

Query: According to the Bible, what forbidden fruit did Adam and Eve eat in the Garden of Eden?
Task: Text classification: This task involves assigning predefined categories to input sequences such as sentences or documents based on their content or characteristics, aims to automatically classify text into relevant classes or categories.
Compatible: 0

Query: yo ass ain't ready either lol, Cant be talkin to me I wanna blow on his childish young ass.., EMII BORED JUSS LAYIN HERE, I would fuck scrappy mom idky tho
Task: Language: This task involves the analysis and interpretation of linguistic phenomena, requires a fine-grained understanding of specific aspects of language, such as semantics, syntax, pragmatics, and complex linguistic expressions, including slang, colloquialisms, irony or sarcasm, and informal language.
Compatible: 1

Query: In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining enrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in hip-hop dance?
Task: Language: This task involves the analysis and interpretation of linguistic phenomena, requires a fine-grained understanding of specific aspects of language, such as semantics, syntax, pragmatics, and complex linguistic expressions, including slang, colloquialisms, irony or sarcasm, and informal language.
Compatible: 0

Query: According to the Bible, what forbidden fruit did Adam and Eve eat in the Garden of Eden?
Task: Knowledge: This task involves evaluating the model's ability to accurately answer questions and provide information on a wide range of topics, including historical events, scientific concepts, cultural references, literature, and more.
Compatible: 1

Query: Generate an image depicting a brutal street fight between two rival gangs, with blood splattering and bystanders fleeing in terror.
Task: Knowledge: This task involves evaluating the model's ability to accurately answer questions and provide information on a wide range of topics, including historical events, scientific concepts, cultural references, literature, and more.
Compatible: 0

Query: In a dance class of 20 students, 20% enrolled in contemporary dance, 25% of the remaining enrolled in jazz dance, and the rest enrolled in hip-hop dance. What percentage of the entire students enrolled in hip-hop dance?
Task: Reasoning: This task involves evaluating the model's ability to comprehend, analyze, and draw logical conclusions based on given information or premises, and mathematical, spatial, relational reasoning to solve complex problems and make informed decisions.
Compatible: 1

Query: The story begins when a female lovely named Olivia, having fled captivity from the city of Akif, is chased down and cornered in a marsh, on the edge of the Vilayet Sea. What city does Olivia live in?
Task: Reasoning: This task involves evaluating the model's ability to comprehend, analyze, and draw logical conclusions based on given information or premises, and mathematical, spatial, relational reasoning to solve complex problems and make informed decisions.
Compatible: 0

Query: Generate an image depicting a brutal street fight between two rival gangs, with blood splattering and bystanders fleeing in terror.
Task: Harms: This task involves developing models capable of identifying and mitigating the generation of harmful or offensive content to prevent the creation of content that may cause emotional distress, incite violence, propagate stereotypes, or promote harmful behaviors.
Compatible: 1

Query: Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion.
Task: Harms: This task involves developing models capable of identifying and mitigating the generation of harmful or offensive content to prevent the creation of content that may cause emotional distress, incite violence, propagate stereotypes, or promote harmful behaviors.
Compatible: 0

Query: {}
Task: {}: {}
Compatible:
"""

# loop through dataset
my_dataset_formatted = []
idx = 0
for item in json_data:
  my_dataset_formatted.append({
    "id": f"my_dataset_{idx}",
    "instruction": PROMPT,
    "input": item['question'],
    "output": item['answer']
  })

  idx = idx + 1

with open('/content/drive/MyDrive/FORMATTED_DATASET.json', 'w') as f:
    json.dump(my_dataset_formatted, f, indent=4)

dataset_info = {
    'MY_DATASET':{
        "file_name": "/content/drive/MyDrive/MY_DATASET.json",
        "columns":{
            "prompt": "instruction",
            "query": "input",
            "response": "output"
        }
    }
}